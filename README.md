# Question Answering System with Vector Database
This project implements a question-answering system that utilizes a vector database (Qdrant) to store answers and their corresponding embeddings, using Hugging Face model for embedding calculation and answering queries.

There is a development_notebook.ipynb file containing the development process and evaluation, as well as an app.py script that can be used to test the solution (remember to uncomment the correct insert-related line, lines 123-128). It also contains three example questions (lines 130-133).

The main assumption is that since the time and resources for solving this task are strongly limited, as well as the tools that can be used (using LLM APIs is not allowed - hence RAG with LLM solution is not allowed as well), the objective of this task is to test whether I understand the problem and can propose a solution for it, rather than trying to deliver high-quality answers.

# Description
The system provides an efficient way to search for relevant answers from a large dataset by:

1. Generating embeddings for text (questions and answers) using pre-trained models from Hugging Face.
2. Storing answers in a Qdrant vector database to enable efficient similarity searches.
3. Retrieving the most relevant answer to a given query by ranking results using cosine similarity.
4. Using a question-answering pipeline from Hugging Face to extract precise answers from the retrieved text chunks.

# Usage
1. Create Vector Database
Create an object of the VectorDatabase class and insert a list of answers (documents that need to be used for search) into it using the insert_answers() method.

3. Search Vector Database
Search for the most relevant answers using the search_answer() method, which takes query (the question that needs to be answered) as an argument. This method returns the top k most relevant documents, where k is adjustable (default: 3).

4. Create an object of the QuestionAnsweringModel class, which uses a Hugging Face model to answer the question based on the most relevant documents from the previous step. Use the get_best_result() method, which takes query (the question that needs to be answered) and search_results (the most relevant documents from the previous step) as arguments. By default, it uses a threshold of 0.3, which can be adjusted via the min_score argument.

The method returns a dictionary containing:

- The answer generated by the model
- The score indicating the model’s confidence in the answer
- The document chunk used for prediction
- The start and end indices for locating the answer within the chunk
- The full document to which the chunk belongs**

# Evaluation
The first approach to evaluation is to check what percentage of generated answers was based on the same "answer" (document) as in the provided dataset. This is because, even if the generated answer is not informative enough, the user can still be given the option to check the entire text, which may be sufficient. The metric used is "exact match accuracy", which, with the default threshold, was around 24% for the entire sample and 30% for answers where the model had high confidence (among non-"I don't know" answers).

Considering that fine-tuning was skipped, the document database is very large, and a relatively simple question-answering model was used, this accuracy is relatively good. With more time, we could likely achieve even better results, even with this simple approach.

The advantage of this metric is that it is easily interpretable (informative for non-math stakeholders) and provides a good approximation of solution quality. However, its main drawback is that it does not indicate whether a "non-exact match" was still close to the correct answer.

To address this limitation, I manually reviewed cases where the model made mistakes. In many of these cases, fine-tuning would likely improve performance, as the model was often misled by general text similarity, such as slight differences in disease names. Additionally, some incorrect answers were still relatively close to the correct one—for example, the generated answer referred to the right disease but was not an exact match.

# General idea behind the solution & how to improve the solution
As I said in the beginning the main assumption is that the objective is to test whether I understand the problem and can propose a solution rather than focusing on delivering high-quality answers.

For that reason, I skipped fine-tuning the embedding model used for the Vector Database. In highly specialized cases like this one (medical data), fine-tuning is usually crucial, as embedding models are trained on general data sources and may not capture all domain-specific details (which, according to the evaluation, is a limitation of my solution).

Since no fine-tuning or model training was performed, there was no risk of overfitting, and a train-test split was not necessary. Underfitting is certainly present, but this was an accepted trade-off, as explained earlier. In a real-world scenario, this step would be mandatory, as fine-tuning would not be skipped.

Typically, in cases like this, using RAG with the ChatGPT API or another LLM would be the obvious choice unless the stakeholder has a strong reason against it (e.g., privacy concerns, costs, legal constraints). However in this case it was not allowed. Another possible approach would be to use Hugging Face models based on the GPT architecture. Another method that could significantly improve results is Question Expansion/Reformulation (RAG approach).

Hereby I confirm that I did not use AI assistance.